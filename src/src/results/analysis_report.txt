RNN Hyperparameter Analysis Report
==================================================

1. Effect of RNN Layer Count
------------------------------
Results:
  1 layers: Test F1 = 0.1333, Valid F1 = 0.0000
  2 layers: Test F1 = 0.0000, Valid F1 = 0.0000
  3 layers: Test F1 = 0.1333, Valid F1 = 0.3000

Best configuration: 1 layers
Best Test F1 Score: 0.1333
Trend: Performance appears to be stable across different layer counts.

2. Effect of RNN Hidden Units
------------------------------
Results:
  32 units: Test F1 = 0.3333, Valid F1 = 0.3889
  64 units: Test F1 = 0.3333, Valid F1 = 0.2222
  128 units: Test F1 = 0.0000, Valid F1 = 0.1333

Best configuration: 32 units
Best Test F1 Score: 0.3333

3. Effect of Bidirectional RNN
------------------------------
  Unidirectional: Test F1 = 0.6667, Valid F1 = 0.1667
  Bidirectional: Test F1 = 0.1111, Valid F1 = 0.0000

Unidirectional RNN performs 500.0% better than bidirectional.

4. Overall Conclusions
------------------------------
Best overall configuration:
  - Experiment: bidirectional_False
  - Configuration: {'vocab_size': 42, 'embedding_dim': 64, 'rnn_units': 32, 'num_classes': 3, 'num_rnn_layers': 1, 'bidirectional': False, 'dropout_rate': 0.2, 'activation': 'tanh', 'max_length': 50, 'epochs': 5, 'batch_size': 32, 'learning_rate': 0.001}
  - Test F1 Score: 0.6667
  - Test Accuracy: 0.6000

5. Recommendations
------------------------------
Based on the experimental results:
  - Use 1 RNN layer(s) for optimal performance
  - Use 32 hidden units for best results
  - Use unidirectional RNN architecture