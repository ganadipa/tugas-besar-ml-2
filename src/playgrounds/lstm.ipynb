{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8eb9e974",
   "metadata": {},
   "source": [
    "# 1. IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "822aabfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-30 08:48:37.746385: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-30 08:48:37.746852: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-30 08:48:37.749525: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-30 08:48:37.757345: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748569717.769981  545260 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748569717.773788  545260 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1748569717.783225  545260 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748569717.783247  545260 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748569717.783249  545260 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748569717.783250  545260 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-30 08:48:37.786328: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starter has been initialized.\n"
     ]
    }
   ],
   "source": [
    "import os,pandas,sys,time,keras,json,sklearn,tensorflow,random\n",
    "import numpy as np\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "os.environ['PYTHONHASHSEED']='0'\n",
    "\n",
    "def reseed():\n",
    "    seed = 0x5f3759df\n",
    "    np.random.seed(seed)\n",
    "    tensorflow.random.set_seed(seed)\n",
    "    keras.utils.set_random_seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "from starter import Starter\n",
    "starter = Starter()\n",
    "starter.start(lambda: os.chdir(os.path.dirname(os.getcwd())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcd26eb",
   "metadata": {},
   "source": [
    "# 2. LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a57321f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. LOADING AND PREPROCESSING DATA\n",
      "==================================================\n",
      "Loaded data:\n",
      "  Train: 500 samples\n",
      "  Valid: 100 samples\n",
      "  Test: 400 samples\n",
      "Keras vectorizer vocabulary size: 2836\n",
      "Sample vocabulary: ['', '[UNK]', np.str_('yang'), np.str_('di'), np.str_('dan'), np.str_('tidak'), np.str_('saya'), np.str_('dengan'), np.str_('enak'), np.str_('ini')]\n",
      "\n",
      "Data shapes:\n",
      "  X_train: (500, 50)\n",
      "  y_train: (500,)\n",
      "  X_valid: (100, 50)\n",
      "  y_valid: (100,)\n",
      "  X_test: (400, 50)\n",
      "  y_test: (400,)\n",
      "\n",
      "Data loaded successfully!\n",
      "Vocabulary size: 2836\n",
      "Number of classes: 3\n",
      "Max sequence length: 50\n",
      "\n",
      "Class distribution in training data:\n",
      "  negative: 192 (38.4%)\n",
      "  neutral: 119 (23.8%)\n",
      "  positive: 189 (37.8%)\n",
      "\n",
      "Data shapes:\n",
      "  Training: X=(500, 50), y=(500,)\n",
      "  Validation: X=(100, 50), y=(100,)\n",
      "  Test: X=(400, 50), y=(400,)\n",
      "\n",
      "Sample data:\n",
      "  First training text tokens: [1758 1080 1145  196 2834  198   11  607  177  847]...\n",
      "  First training label: 1 (neutral)\n",
      "\n",
      "Data integrity checks:\n",
      "  No missing values in X_train: True\n",
      "  No missing values in y_train: True\n",
      "  All labels in valid range: True\n",
      "\n",
      "Data preprocessing completed successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-30 08:48:39.528996: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "from utils.data_loader import DataLoader\n",
    "import numpy as np \n",
    "\n",
    "print(\"1. LOADING AND PREPROCESSING DATA\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Initialize data loader\n",
    "data_dir = \"../data\" \n",
    "data_loader = DataLoader(data_dir)\n",
    "\n",
    "# Load and prepare data\n",
    "try:\n",
    "    X_train, y_train, X_valid, y_valid, X_test, y_test = data_loader.prepare_data(\n",
    "        max_vocab_size=5000,\n",
    "        max_length=50,\n",
    "    )\n",
    "    \n",
    "    print(\"\\nData loaded successfully!\")\n",
    "    print(f\"Vocabulary size: {data_loader.preprocessor.vocab_size}\")\n",
    "    print(f\"Number of classes: {data_loader.num_classes}\")\n",
    "    print(f\"Max sequence length: {data_loader.preprocessor.max_length}\")\n",
    "    \n",
    "    # Show class distribution\n",
    "    unique, counts = np.unique(y_train, return_counts=True)\n",
    "    print(\"\\nClass distribution in training data:\")\n",
    "    for class_id, count in zip(unique, counts):\n",
    "        class_name = data_loader.reverse_label_encoder[class_id]\n",
    "        print(f\"  {class_name}: {count} ({count/len(y_train)*100:.1f}%)\")\n",
    "        \n",
    "    print(\"\\nData shapes:\")\n",
    "    print(f\"  Training: X={X_train.shape}, y={y_train.shape}\")\n",
    "    print(f\"  Validation: X={X_valid.shape}, y={y_valid.shape}\")\n",
    "    print(f\"  Test: X={X_test.shape}, y={y_test.shape}\")\n",
    "    \n",
    "    # Display sample data\n",
    "    print(\"\\nSample data:\")\n",
    "    print(f\"  First training text tokens: {X_train[0][:10]}...\")\n",
    "    print(f\"  First training label: {y_train[0]} ({data_loader.reverse_label_encoder[y_train[0]]})\")\n",
    "    \n",
    "    # Verify data integrity\n",
    "    print(\"\\nData integrity checks:\")\n",
    "    print(f\"  No missing values in X_train: {not np.any(np.isnan(X_train))}\")\n",
    "    print(f\"  No missing values in y_train: {not np.any(np.isnan(y_train))}\")\n",
    "    print(f\"  All labels in valid range: {np.all((y_train >= 0) & (y_train < data_loader.num_classes))}\")\n",
    "    \n",
    "    print(\"\\nData preprocessing completed successfully!\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da1d094",
   "metadata": {},
   "source": [
    "# 3. INITIALIZE EXPERIMENT RUNNER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c66dde9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Keras experiment framework...\n",
      "Base configuration:\n",
      "  vocab_size: 2836\n",
      "  embedding_dim: 64\n",
      "  lstm_units: 32\n",
      "  num_classes: 3\n",
      "  max_length: 50\n",
      "  activation: tanh\n",
      "  dropout_rate: 0.2\n",
      "  learning_rate: 0.001\n",
      "  batch_size: 32\n",
      "  epochs: 15\n",
      "Keras experiment runner initialized!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import keras\n",
    "import sklearn.metrics\n",
    "\n",
    "\n",
    "class KerasLSTMExperiment:\n",
    "    \"\"\"Keras LSTM experiment class for systematic hyperparameter analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, data_loader, X_train, y_train, X_valid, y_valid, X_test, y_test):\n",
    "        self.data_loader = data_loader\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_valid = X_valid\n",
    "        self.y_valid = y_valid\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        \n",
    "        # Base configuration\n",
    "        self.base_config = {\n",
    "            'vocab_size': data_loader.preprocessor.vocab_size,\n",
    "            'embedding_dim': 64,\n",
    "            'lstm_units': 32,\n",
    "            'num_classes': data_loader.num_classes,\n",
    "            'max_length': data_loader.preprocessor.max_length,\n",
    "            'activation': 'tanh',\n",
    "            'dropout_rate': 0.2,\n",
    "            'learning_rate': 0.001,\n",
    "            'batch_size': 32,\n",
    "            'epochs': 15\n",
    "        }\n",
    "        \n",
    "        print(f\"Base configuration:\")\n",
    "        for key, value in self.base_config.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "    \n",
    "    def create_keras_model(self, config):\n",
    "        \"\"\"Create Keras lstm model with given configuration\"\"\"\n",
    "        reseed()\n",
    "        model = keras.models.Sequential()\n",
    "        model.add(keras.layers.Embedding(\n",
    "            input_dim=config['vocab_size'],\n",
    "            output_dim=config['embedding_dim'],\n",
    "            input_length=config['max_length'],\n",
    "            name='embedding'\n",
    "        ))\n",
    "        \n",
    "        for i in range(config['num_lstm_layers']):\n",
    "            return_sequences = i < config['num_lstm_layers'] - 1\n",
    "            lstm_layer = keras.layers.LSTM(\n",
    "                units=config['lstm_units'],\n",
    "                activation=config['activation'],\n",
    "                return_sequences=return_sequences,\n",
    "                name=f'lstm_{i}'\n",
    "            )\n",
    "            \n",
    "            if config['bidirectional']:\n",
    "                model.add(keras.layers.Bidirectional(lstm_layer, name=f'bidirectional_lstm_{i}'))\n",
    "            else:\n",
    "                model.add(lstm_layer)\n",
    "            \n",
    "            if i < config['num_lstm_layers'] - 1:\n",
    "                model.add(keras.layers.Dropout(config['dropout_rate'], name=f'dropout_{i}'))\n",
    "        \n",
    "        model.add(keras.layers.Dropout(config['dropout_rate'], name='dropout_final'))\n",
    "        model.add(keras.layers.Dense(config['num_classes'], activation='softmax', name='classification'))\n",
    "        \n",
    "        model.build(input_shape=(None, config['max_length']))\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=config['learning_rate']),\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def train_and_evaluate(self, config, experiment_name):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Training: {experiment_name}\")\n",
    "        print(f\"Config: {config}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        model = self.create_keras_model(config)\n",
    "        print(f\"Model created with {model.count_params():,} parameters\")\n",
    "        \n",
    "        print(f\"Starting training for {config['epochs']} epochs...\")\n",
    "        history = model.fit(\n",
    "            self.X_train, self.y_train,\n",
    "            validation_data=(self.X_valid, self.y_valid),\n",
    "            epochs=config['epochs'],\n",
    "            batch_size=config['batch_size'],\n",
    "            verbose=1,\n",
    "            callbacks=[\n",
    "                keras.callbacks.EarlyStopping(\n",
    "                    monitor='val_loss',\n",
    "                    patience=5,\n",
    "                    restore_best_weights=True\n",
    "                )\n",
    "            ],\n",
    "            shuffle=False\n",
    "        )\n",
    "        \n",
    "        print(f\"Evaluating on test set...\")\n",
    "        test_loss, test_acc = model.evaluate(self.X_test, self.y_test, verbose=0)\n",
    "        test_predictions = model.predict(self.X_test, verbose=0)\n",
    "        test_pred_classes = np.argmax(test_predictions, axis=1)\n",
    "        test_f1_macro = sklearn.metrics.f1_score(self.y_test, test_pred_classes, average='macro')\n",
    "        \n",
    "        valid_predictions = model.predict(self.X_valid, verbose=0)\n",
    "        valid_pred_classes = np.argmax(valid_predictions, axis=1)\n",
    "        valid_f1_macro = sklearn.metrics.f1_score(self.y_valid, valid_pred_classes, average='macro')\n",
    "        \n",
    "        weights_path = f\"results/{experiment_name}_weights.npz\"\n",
    "        os.makedirs(\"results\", exist_ok=True)\n",
    "        self.save_keras_weights(model, weights_path, config)\n",
    "\n",
    "        unique_rows = np.unique(self.X_test, axis=0)\n",
    "\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"\\nResults for {experiment_name}:\")\n",
    "        print(f\"  Training time: {training_time:.2f} seconds\")\n",
    "        print(f\"  Test Accuracy: {test_acc:.4f}\")\n",
    "        print(f\"  Test F1-Score (macro): {test_f1_macro:.4f}\")\n",
    "        print(f\"  Valid F1-Score (macro): {valid_f1_macro:.4f}\")\n",
    "        print(f\"  Weights saved to: {weights_path}\")\n",
    "        \n",
    "        return {\n",
    "            'model': model,\n",
    "            'history': history.history,\n",
    "            'test_accuracy': test_acc,\n",
    "            'test_f1_score': test_f1_macro,\n",
    "            'valid_f1_score': valid_f1_macro,\n",
    "            'weights_path': weights_path,\n",
    "            'config': config,\n",
    "            'training_time': training_time\n",
    "        }\n",
    "    \n",
    "    def save_keras_weights(self, model, filepath, config):\n",
    "        print(f\"Saving Keras weights to: {filepath}\")\n",
    "        \n",
    "        try:\n",
    "            if len(model.weights) == 0:\n",
    "                raise ValueError(\"Model has no weights to save!\")\n",
    "            \n",
    "            weights_dict = {}\n",
    "            lstm_layer_count = 0\n",
    "            \n",
    "            for layer in model.layers:\n",
    "                layer_weights = layer.get_weights()\n",
    "                if len(layer_weights) == 0:\n",
    "                    continue\n",
    "                    \n",
    "                layer_name = layer.name\n",
    "                print(f\"  Processing layer: {layer_name} - {len(layer_weights)} weight arrays\")\n",
    "                \n",
    "                if 'embedding' in layer_name.lower():\n",
    "                    weights_dict['embedding'] = {\n",
    "                        'embedding_matrix': layer_weights[0]\n",
    "                    }\n",
    "                elif 'simple_lstm' in layer_name.lower():\n",
    "                    target_name = f'lstm_{lstm_layer_count}'\n",
    "                    weights_dict[target_name] = {\n",
    "                        'W_ih': layer_weights[0].T,\n",
    "                        'W_hh': layer_weights[1].T,\n",
    "                        'b_h': layer_weights[2]\n",
    "                    }\n",
    "                    lstm_layer_count += 1\n",
    "                elif 'bidirectional' in layer_name.lower():\n",
    "                    target_name = f'bidirectional_lstm_{lstm_layer_count}'\n",
    "                    if len(layer_weights) >= 6:\n",
    "                        weights_dict[target_name] = {\n",
    "                            'forward_W_ih': layer_weights[0].T,\n",
    "                            'forward_W_hh': layer_weights[1].T,\n",
    "                            'forward_b_h': layer_weights[2],\n",
    "                            'backward_W_ih': layer_weights[3].T,\n",
    "                            'backward_W_hh': layer_weights[4].T,\n",
    "                            'backward_b_h': layer_weights[5]\n",
    "                        }\n",
    "                    lstm_layer_count += 1\n",
    "                elif 'dense' in layer_name.lower() or 'classification' in layer_name.lower():\n",
    "                    weights_dict['classification'] = {\n",
    "                        'W': layer_weights[0].T,\n",
    "                        'b': layer_weights[1]\n",
    "                    }\n",
    "            \n",
    "            save_dict = {}\n",
    "            for layer_name, layer_weights in weights_dict.items():\n",
    "                for weight_name, weight_value in layer_weights.items():\n",
    "                    save_dict[f\"{layer_name}_{weight_name}\"] = weight_value\n",
    "            \n",
    "            save_dict['config'] = json.dumps(config)\n",
    "            np.savez(filepath, **save_dict)\n",
    "            print(f\"  Saved {len(save_dict)-1} weight arrays successfully\")\n",
    "            print(f\"  Layers saved: {list(weights_dict.keys())}\")\n",
    "            \n",
    "            loaded_check = np.load(filepath)\n",
    "            assert len(loaded_check.files) == len(save_dict), \"Save verification failed\"\n",
    "            loaded_check.close()\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error saving weights: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            raise\n",
    "\n",
    "\n",
    "print(\"Initializing Keras experiment framework...\")\n",
    "keras_experiment = KerasLSTMExperiment(\n",
    "    data_loader, X_train, y_train, X_valid, y_valid, X_test, y_test\n",
    ")\n",
    "print(\"Keras experiment runner initialized!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8713bb",
   "metadata": {},
   "source": [
    "# 4. EXPERIMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96414df",
   "metadata": {},
   "source": [
    "## 4.1. Comparing Layer Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d995b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔬 Experiment: lstm_layers_1\n",
      "   Layers: 1, Units: 64, Bidirectional: False\n",
      "\n",
      "============================================================\n",
      "Training: lstm_layers_1\n",
      "Config: {'vocab_size': 2836, 'embedding_dim': 64, 'lstm_units': 64, 'num_classes': 3, 'max_length': 50, 'activation': 'tanh', 'dropout_rate': 0.2, 'learning_rate': 0.001, 'batch_size': 32, 'epochs': 15, 'num_lstm_layers': 1, 'bidirectional': False}\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ganadipa/code/kuliah/sem6/Machine-learning/Tugas-Besar-II-ML/.venv/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created with 214,723 parameters\n",
      "Starting training for 15 epochs...\n",
      "Epoch 1/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.3686 - loss: 1.0971 - val_accuracy: 0.3700 - val_loss: 1.0841\n",
      "Epoch 2/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.3950 - loss: 1.0878 - val_accuracy: 0.3800 - val_loss: 1.0792\n",
      "Epoch 3/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4254 - loss: 1.0737 - val_accuracy: 0.5100 - val_loss: 1.0066\n",
      "Epoch 4/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4726 - loss: 0.9811 - val_accuracy: 0.5200 - val_loss: 0.9588\n",
      "Epoch 5/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5746 - loss: 0.8601 - val_accuracy: 0.6000 - val_loss: 0.8726\n",
      "Epoch 6/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6684 - loss: 0.6973 - val_accuracy: 0.6000 - val_loss: 0.9774\n",
      "Epoch 7/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7064 - loss: 0.5931 - val_accuracy: 0.6000 - val_loss: 0.8925\n",
      "Epoch 8/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7325 - loss: 0.5390 - val_accuracy: 0.5300 - val_loss: 1.2004\n",
      "Epoch 9/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6833 - loss: 0.6902 - val_accuracy: 0.5600 - val_loss: 1.4582\n",
      "Epoch 10/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7455 - loss: 0.5740 - val_accuracy: 0.5600 - val_loss: 1.1353\n",
      "Evaluating on test set...\n",
      "Saving Keras weights to: results/lstm_layers_1_weights.npz\n",
      "  Processing layer: embedding - 1 weight arrays\n",
      "  Processing layer: lstm_0 - 3 weight arrays\n",
      "  Processing layer: classification - 2 weight arrays\n",
      "  Saved 3 weight arrays successfully\n",
      "  Layers saved: ['embedding', 'classification']\n",
      "\n",
      "Results for lstm_layers_1:\n",
      "  Training time: 3.70 seconds\n",
      "  Test Accuracy: 0.5950\n",
      "  Test F1-Score (macro): 0.4574\n",
      "  Valid F1-Score (macro): 0.4630\n",
      "  Weights saved to: results/lstm_layers_1_weights.npz\n",
      "model <Sequential name=sequential, built=True>\n",
      "history {'accuracy': [0.3840000033378601, 0.41200000047683716, 0.44600000977516174, 0.4519999921321869, 0.5820000171661377, 0.671999990940094, 0.7099999785423279, 0.7319999933242798, 0.6940000057220459, 0.7300000190734863], 'loss': [1.0915066003799438, 1.0823439359664917, 1.0623334646224976, 1.0083619356155396, 0.8492242693901062, 0.6995109915733337, 0.5909494757652283, 0.5501575469970703, 0.6642650961875916, 0.6244558095932007], 'val_accuracy': [0.3700000047683716, 0.3799999952316284, 0.5099999904632568, 0.5199999809265137, 0.6000000238418579, 0.6000000238418579, 0.6000000238418579, 0.5299999713897705, 0.5600000023841858, 0.5600000023841858], 'val_loss': [1.0841014385223389, 1.0792055130004883, 1.0066025257110596, 0.9588080048561096, 0.8726492524147034, 0.9774229526519775, 0.8924548625946045, 1.2004330158233643, 1.4581701755523682, 1.1353271007537842]}\n",
      "test_accuracy 0.5950000286102295\n",
      "test_f1_score 0.4573569912862883\n",
      "valid_f1_score 0.4630449362843729\n",
      "weights_path results/lstm_layers_1_weights.npz\n",
      "config {'vocab_size': 2836, 'embedding_dim': 64, 'lstm_units': 64, 'num_classes': 3, 'max_length': 50, 'activation': 'tanh', 'dropout_rate': 0.2, 'learning_rate': 0.001, 'batch_size': 32, 'epochs': 15, 'num_lstm_layers': 1, 'bidirectional': False}\n",
      "training_time 3.6971230506896973\n",
      " lstm_layers_1 completed successfully!\n",
      "\n",
      "🔬 Experiment: lstm_layers_3\n",
      "   Layers: 3, Units: 64, Bidirectional: False\n",
      "\n",
      "============================================================\n",
      "Training: lstm_layers_3\n",
      "Config: {'vocab_size': 2836, 'embedding_dim': 64, 'lstm_units': 64, 'num_classes': 3, 'max_length': 50, 'activation': 'tanh', 'dropout_rate': 0.2, 'learning_rate': 0.001, 'batch_size': 32, 'epochs': 15, 'num_lstm_layers': 3, 'bidirectional': False}\n",
      "============================================================\n",
      "Model created with 280,771 parameters\n",
      "Starting training for 15 epochs...\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ganadipa/code/kuliah/sem6/Machine-learning/Tugas-Besar-II-ML/.venv/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 48ms/step - accuracy: 0.4059 - loss: 1.0929 - val_accuracy: 0.3800 - val_loss: 1.0770\n",
      "Epoch 2/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.3699 - loss: 1.0499 - val_accuracy: 0.5000 - val_loss: 0.9580\n",
      "Epoch 3/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.5716 - loss: 0.8882 - val_accuracy: 0.5600 - val_loss: 0.9136\n",
      "Epoch 4/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.6477 - loss: 0.7373 - val_accuracy: 0.6500 - val_loss: 0.8700\n",
      "Epoch 5/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.7814 - loss: 0.5313 - val_accuracy: 0.6600 - val_loss: 0.9148\n",
      "Epoch 6/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8550 - loss: 0.4072 - val_accuracy: 0.6400 - val_loss: 1.0606\n",
      "Epoch 7/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8871 - loss: 0.3785 - val_accuracy: 0.6100 - val_loss: 1.0191\n",
      "Epoch 8/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9223 - loss: 0.2769 - val_accuracy: 0.5900 - val_loss: 1.1899\n",
      "Epoch 9/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9372 - loss: 0.2489 - val_accuracy: 0.5600 - val_loss: 1.4383\n",
      "Evaluating on test set...\n",
      "Saving Keras weights to: results/lstm_layers_3_weights.npz\n",
      "  Processing layer: embedding - 1 weight arrays\n",
      "  Processing layer: lstm_0 - 3 weight arrays\n",
      "  Processing layer: lstm_1 - 3 weight arrays\n",
      "  Processing layer: lstm_2 - 3 weight arrays\n",
      "  Processing layer: classification - 2 weight arrays\n",
      "  Saved 3 weight arrays successfully\n",
      "  Layers saved: ['embedding', 'classification']\n",
      "\n",
      "Results for lstm_layers_3:\n",
      "  Training time: 7.68 seconds\n",
      "  Test Accuracy: 0.6350\n",
      "  Test F1-Score (macro): 0.5935\n",
      "  Valid F1-Score (macro): 0.6232\n",
      "  Weights saved to: results/lstm_layers_3_weights.npz\n",
      "model <Sequential name=sequential_1, built=True>\n",
      "history {'accuracy': [0.41200000047683716, 0.42399999499320984, 0.5519999861717224, 0.6800000071525574, 0.7919999957084656, 0.8220000267028809, 0.8740000128746033, 0.9160000085830688, 0.9419999718666077], 'loss': [1.0862151384353638, 1.0018746852874756, 0.8950885534286499, 0.7064885497093201, 0.5158492922782898, 0.4727665185928345, 0.39294761419296265, 0.28395581245422363, 0.21651077270507812], 'val_accuracy': [0.3799999952316284, 0.5, 0.5600000023841858, 0.6499999761581421, 0.6600000262260437, 0.6399999856948853, 0.6100000143051147, 0.5899999737739563, 0.5600000023841858], 'val_loss': [1.0769751071929932, 0.9579989910125732, 0.9135550856590271, 0.8699586987495422, 0.9147797226905823, 1.0605590343475342, 1.0191450119018555, 1.1898733377456665, 1.4383283853530884]}\n",
      "test_accuracy 0.6349999904632568\n",
      "test_f1_score 0.5935051113522323\n",
      "valid_f1_score 0.6231722631722633\n",
      "weights_path results/lstm_layers_3_weights.npz\n",
      "config {'vocab_size': 2836, 'embedding_dim': 64, 'lstm_units': 64, 'num_classes': 3, 'max_length': 50, 'activation': 'tanh', 'dropout_rate': 0.2, 'learning_rate': 0.001, 'batch_size': 32, 'epochs': 15, 'num_lstm_layers': 3, 'bidirectional': False}\n",
      "training_time 7.677304029464722\n",
      " lstm_layers_3 completed successfully!\n",
      "\n",
      "🔬 Experiment: lstm_layers_5\n",
      "   Layers: 5, Units: 64, Bidirectional: False\n",
      "\n",
      "============================================================\n",
      "Training: lstm_layers_5\n",
      "Config: {'vocab_size': 2836, 'embedding_dim': 64, 'lstm_units': 64, 'num_classes': 3, 'max_length': 50, 'activation': 'tanh', 'dropout_rate': 0.2, 'learning_rate': 0.001, 'batch_size': 32, 'epochs': 15, 'num_lstm_layers': 5, 'bidirectional': False}\n",
      "============================================================\n",
      "Model created with 346,819 parameters\n",
      "Starting training for 15 epochs...\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ganadipa/code/kuliah/sem6/Machine-learning/Tugas-Besar-II-ML/.venv/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 80ms/step - accuracy: 0.4156 - loss: 1.0746 - val_accuracy: 0.4900 - val_loss: 1.0502\n",
      "Epoch 2/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.5665 - loss: 0.9044 - val_accuracy: 0.5700 - val_loss: 0.9086\n",
      "Epoch 3/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.6886 - loss: 0.7088 - val_accuracy: 0.5500 - val_loss: 0.8645\n",
      "Epoch 4/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8012 - loss: 0.4902 - val_accuracy: 0.6500 - val_loss: 0.9746\n",
      "Epoch 5/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9281 - loss: 0.2654 - val_accuracy: 0.5700 - val_loss: 1.2943\n",
      "Epoch 6/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9350 - loss: 0.1851 - val_accuracy: 0.5800 - val_loss: 1.2969\n",
      "Epoch 7/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9199 - loss: 0.2671 - val_accuracy: 0.5600 - val_loss: 1.1834\n",
      "Epoch 8/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9126 - loss: 0.2424 - val_accuracy: 0.6300 - val_loss: 1.3431\n",
      "Evaluating on test set...\n",
      "Saving Keras weights to: results/lstm_layers_5_weights.npz\n",
      "  Processing layer: embedding - 1 weight arrays\n",
      "  Processing layer: lstm_0 - 3 weight arrays\n",
      "  Processing layer: lstm_1 - 3 weight arrays\n",
      "  Processing layer: lstm_2 - 3 weight arrays\n",
      "  Processing layer: lstm_3 - 3 weight arrays\n",
      "  Processing layer: lstm_4 - 3 weight arrays\n",
      "  Processing layer: classification - 2 weight arrays\n",
      "  Saved 3 weight arrays successfully\n",
      "  Layers saved: ['embedding', 'classification']\n",
      "\n",
      "Results for lstm_layers_5:\n",
      "  Training time: 11.34 seconds\n",
      "  Test Accuracy: 0.5750\n",
      "  Test F1-Score (macro): 0.4893\n",
      "  Valid F1-Score (macro): 0.4431\n",
      "  Weights saved to: results/lstm_layers_5_weights.npz\n",
      "model <Sequential name=sequential_2, built=True>\n",
      "history {'accuracy': [0.44600000977516174, 0.5580000281333923, 0.6940000057220459, 0.8100000023841858, 0.9259999990463257, 0.9520000219345093, 0.9279999732971191, 0.921999990940094], 'loss': [1.032279133796692, 0.8970323801040649, 0.6780638098716736, 0.47752851247787476, 0.26424530148506165, 0.162275493144989, 0.23956447839736938, 0.24129103124141693], 'val_accuracy': [0.49000000953674316, 0.5699999928474426, 0.550000011920929, 0.6499999761581421, 0.5699999928474426, 0.5799999833106995, 0.5600000023841858, 0.6299999952316284], 'val_loss': [1.050177812576294, 0.9086230993270874, 0.8645319938659668, 0.9746177792549133, 1.2942904233932495, 1.2968980073928833, 1.1834276914596558, 1.343135952949524]}\n",
      "test_accuracy 0.574999988079071\n",
      "test_f1_score 0.48933349916212815\n",
      "valid_f1_score 0.44310300350101345\n",
      "weights_path results/lstm_layers_5_weights.npz\n",
      "config {'vocab_size': 2836, 'embedding_dim': 64, 'lstm_units': 64, 'num_classes': 3, 'max_length': 50, 'activation': 'tanh', 'dropout_rate': 0.2, 'learning_rate': 0.001, 'batch_size': 32, 'epochs': 15, 'num_lstm_layers': 5, 'bidirectional': False}\n",
      "training_time 11.341059446334839\n",
      " lstm_layers_5 completed successfully!\n",
      "Layers   Accuracy   F1-Score   Time (s)  \n",
      "----------------------------------------\n",
      "1        0.5950    0.4574    3.7\n",
      "3        0.6350    0.5935    7.7\n",
      "5        0.5750    0.4893    11.3\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "layer_counts = [1, 3, 5]\n",
    "layer_results = {}\n",
    "\n",
    "for num_layers in layer_counts:\n",
    "    config = keras_experiment.base_config.copy()\n",
    "    config.update({\n",
    "        'num_lstm_layers': num_layers,\n",
    "        'bidirectional': False, \n",
    "        'lstm_units': 64\n",
    "    })\n",
    "    # config['epochs'] = 10\n",
    "    \n",
    "    experiment_name = f\"lstm_layers_{num_layers}\"\n",
    "    print(f\"\\n🔬 Experiment: {experiment_name}\")\n",
    "    print(f\"   Layers: {num_layers}, Units: {config['lstm_units']}, Bidirectional: {config['bidirectional']}\")\n",
    "    \n",
    "    try:\n",
    "        result = keras_experiment.train_and_evaluate(config, experiment_name)\n",
    "        for k,v in result.items():\n",
    "            print(k, v)\n",
    "        layer_results[num_layers] = result\n",
    "        print(f\" {experiment_name} completed successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\" Error in {experiment_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"{'Layers':<8} {'Accuracy':<10} {'F1-Score':<10} {'Time (s)':<10}\")\n",
    "print(\"-\"*40)\n",
    "for layers in sorted(layer_results.keys()):\n",
    "    result = layer_results[layers]\n",
    "    print(f\"{layers:<8} {result['test_accuracy']:.4f}    {result['test_f1_score']:.4f}    {result['training_time']:.1f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e20907",
   "metadata": {},
   "source": [
    "## 4.2. Comparing Cell/Unit Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c505e9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training: lstm_cells_16\n",
      "Config: {'vocab_size': 2836, 'embedding_dim': 64, 'lstm_units': 16, 'num_classes': 3, 'max_length': 50, 'activation': 'tanh', 'dropout_rate': 0.2, 'learning_rate': 0.001, 'batch_size': 32, 'epochs': 15, 'num_lstm_layers': 1, 'bidirectional': False}\n",
      "============================================================\n",
      "Model created with 186,739 parameters\n",
      "Starting training for 15 epochs...\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ganadipa/code/kuliah/sem6/Machine-learning/Tugas-Besar-II-ML/.venv/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.3496 - loss: 1.0965 - val_accuracy: 0.4000 - val_loss: 1.0839\n",
      "Epoch 2/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4052 - loss: 1.0881 - val_accuracy: 0.3900 - val_loss: 1.0824\n",
      "Epoch 3/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4466 - loss: 1.0816 - val_accuracy: 0.4100 - val_loss: 1.0806\n",
      "Epoch 4/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4448 - loss: 1.0719 - val_accuracy: 0.3900 - val_loss: 1.0732\n",
      "Epoch 5/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5014 - loss: 1.0383 - val_accuracy: 0.5900 - val_loss: 0.9055\n",
      "Epoch 6/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6760 - loss: 0.7551 - val_accuracy: 0.6100 - val_loss: 0.8684\n",
      "Epoch 7/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7440 - loss: 0.6075 - val_accuracy: 0.6000 - val_loss: 0.9196\n",
      "Epoch 8/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7308 - loss: 0.5557 - val_accuracy: 0.6100 - val_loss: 0.9306\n",
      "Epoch 9/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7464 - loss: 0.5017 - val_accuracy: 0.6000 - val_loss: 0.9876\n",
      "Epoch 10/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7410 - loss: 0.5290 - val_accuracy: 0.5800 - val_loss: 1.0358\n",
      "Epoch 11/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6930 - loss: 0.6363 - val_accuracy: 0.5800 - val_loss: 1.2021\n",
      "Evaluating on test set...\n",
      "Saving Keras weights to: results/lstm_cells_16_weights.npz\n",
      "  Processing layer: embedding - 1 weight arrays\n",
      "  Processing layer: lstm_0 - 3 weight arrays\n",
      "  Processing layer: classification - 2 weight arrays\n",
      "  Saved 3 weight arrays successfully\n",
      "  Layers saved: ['embedding', 'classification']\n",
      "\n",
      "Results for lstm_cells_16:\n",
      "  Training time: 3.41 seconds\n",
      "  Test Accuracy: 0.6525\n",
      "  Test F1-Score (macro): 0.5013\n",
      "  Valid F1-Score (macro): 0.4679\n",
      "  Weights saved to: results/lstm_cells_16_weights.npz\n",
      " lstm_cells_16 completed successfully!\n",
      "\n",
      "============================================================\n",
      "Training: lstm_cells_32\n",
      "Config: {'vocab_size': 2836, 'embedding_dim': 64, 'lstm_units': 32, 'num_classes': 3, 'max_length': 50, 'activation': 'tanh', 'dropout_rate': 0.2, 'learning_rate': 0.001, 'batch_size': 32, 'epochs': 15, 'num_lstm_layers': 1, 'bidirectional': False}\n",
      "============================================================\n",
      "Model created with 194,019 parameters\n",
      "Starting training for 15 epochs...\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ganadipa/code/kuliah/sem6/Machine-learning/Tugas-Besar-II-ML/.venv/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.3882 - loss: 1.0941 - val_accuracy: 0.3900 - val_loss: 1.0838\n",
      "Epoch 2/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3955 - loss: 1.0875 - val_accuracy: 0.3900 - val_loss: 1.0815\n",
      "Epoch 3/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4054 - loss: 1.0849 - val_accuracy: 0.3700 - val_loss: 1.0772\n",
      "Epoch 4/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4221 - loss: 1.0720 - val_accuracy: 0.4700 - val_loss: 1.0339\n",
      "Epoch 5/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4862 - loss: 0.9631 - val_accuracy: 0.5700 - val_loss: 0.9039\n",
      "Epoch 6/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6119 - loss: 0.7520 - val_accuracy: 0.6100 - val_loss: 0.8598\n",
      "Epoch 7/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7080 - loss: 0.6079 - val_accuracy: 0.6000 - val_loss: 0.9138\n",
      "Epoch 8/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7267 - loss: 0.5548 - val_accuracy: 0.5800 - val_loss: 1.1115\n",
      "Epoch 9/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6383 - loss: 0.8797 - val_accuracy: 0.6000 - val_loss: 0.9554\n",
      "Epoch 10/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7039 - loss: 0.5597 - val_accuracy: 0.6100 - val_loss: 0.9180\n",
      "Epoch 11/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7113 - loss: 0.5833 - val_accuracy: 0.6100 - val_loss: 0.8750\n",
      "Evaluating on test set...\n",
      "Saving Keras weights to: results/lstm_cells_32_weights.npz\n",
      "  Processing layer: embedding - 1 weight arrays\n",
      "  Processing layer: lstm_0 - 3 weight arrays\n",
      "  Processing layer: classification - 2 weight arrays\n",
      "  Saved 3 weight arrays successfully\n",
      "  Layers saved: ['embedding', 'classification']\n",
      "\n",
      "Results for lstm_cells_32:\n",
      "  Training time: 3.45 seconds\n",
      "  Test Accuracy: 0.6200\n",
      "  Test F1-Score (macro): 0.4734\n",
      "  Valid F1-Score (macro): 0.4673\n",
      "  Weights saved to: results/lstm_cells_32_weights.npz\n",
      " lstm_cells_32 completed successfully!\n",
      "\n",
      "============================================================\n",
      "Training: lstm_cells_64\n",
      "Config: {'vocab_size': 2836, 'embedding_dim': 64, 'lstm_units': 64, 'num_classes': 3, 'max_length': 50, 'activation': 'tanh', 'dropout_rate': 0.2, 'learning_rate': 0.001, 'batch_size': 32, 'epochs': 15, 'num_lstm_layers': 1, 'bidirectional': False}\n",
      "============================================================\n",
      "Model created with 214,723 parameters\n",
      "Starting training for 15 epochs...\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ganadipa/code/kuliah/sem6/Machine-learning/Tugas-Besar-II-ML/.venv/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.3686 - loss: 1.0971 - val_accuracy: 0.3700 - val_loss: 1.0841\n",
      "Epoch 2/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.3950 - loss: 1.0878 - val_accuracy: 0.3800 - val_loss: 1.0792\n",
      "Epoch 3/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4254 - loss: 1.0737 - val_accuracy: 0.5100 - val_loss: 1.0066\n",
      "Epoch 4/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.4726 - loss: 0.9811 - val_accuracy: 0.5200 - val_loss: 0.9588\n",
      "Epoch 5/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5746 - loss: 0.8601 - val_accuracy: 0.6000 - val_loss: 0.8726\n",
      "Epoch 6/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6684 - loss: 0.6973 - val_accuracy: 0.6000 - val_loss: 0.9774\n",
      "Epoch 7/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7064 - loss: 0.5931 - val_accuracy: 0.6000 - val_loss: 0.8925\n",
      "Epoch 8/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7325 - loss: 0.5390 - val_accuracy: 0.5300 - val_loss: 1.2004\n",
      "Epoch 9/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6833 - loss: 0.6902 - val_accuracy: 0.5600 - val_loss: 1.4582\n",
      "Epoch 10/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7455 - loss: 0.5740 - val_accuracy: 0.5600 - val_loss: 1.1353\n",
      "Evaluating on test set...\n",
      "Saving Keras weights to: results/lstm_cells_64_weights.npz\n",
      "  Processing layer: embedding - 1 weight arrays\n",
      "  Processing layer: lstm_0 - 3 weight arrays\n",
      "  Processing layer: classification - 2 weight arrays\n",
      "  Saved 3 weight arrays successfully\n",
      "  Layers saved: ['embedding', 'classification']\n",
      "\n",
      "Results for lstm_cells_64:\n",
      "  Training time: 3.80 seconds\n",
      "  Test Accuracy: 0.5950\n",
      "  Test F1-Score (macro): 0.4574\n",
      "  Valid F1-Score (macro): 0.4630\n",
      "  Weights saved to: results/lstm_cells_64_weights.npz\n",
      " lstm_cells_64 completed successfully!\n",
      "\n",
      "============================================================\n",
      "Training: lstm_cells_128\n",
      "Config: {'vocab_size': 2836, 'embedding_dim': 64, 'lstm_units': 128, 'num_classes': 3, 'max_length': 50, 'activation': 'tanh', 'dropout_rate': 0.2, 'learning_rate': 0.001, 'batch_size': 32, 'epochs': 15, 'num_lstm_layers': 1, 'bidirectional': False}\n",
      "============================================================\n",
      "Model created with 280,707 parameters\n",
      "Starting training for 15 epochs...\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ganadipa/code/kuliah/sem6/Machine-learning/Tugas-Besar-II-ML/.venv/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.3743 - loss: 1.0936 - val_accuracy: 0.3800 - val_loss: 1.0846\n",
      "Epoch 2/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.3695 - loss: 1.0875 - val_accuracy: 0.3700 - val_loss: 1.0692\n",
      "Epoch 3/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.4505 - loss: 1.0332 - val_accuracy: 0.5100 - val_loss: 0.9996\n",
      "Epoch 4/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5284 - loss: 0.9125 - val_accuracy: 0.5600 - val_loss: 0.8869\n",
      "Epoch 5/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6854 - loss: 0.6969 - val_accuracy: 0.6100 - val_loss: 0.8480\n",
      "Epoch 6/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7049 - loss: 0.5518 - val_accuracy: 0.6000 - val_loss: 0.8907\n",
      "Epoch 7/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7685 - loss: 0.4925 - val_accuracy: 0.6200 - val_loss: 0.8748\n",
      "Epoch 8/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8832 - loss: 0.3885 - val_accuracy: 0.4300 - val_loss: 1.4936\n",
      "Epoch 9/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7799 - loss: 0.5463 - val_accuracy: 0.5500 - val_loss: 1.1122\n",
      "Epoch 10/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9256 - loss: 0.3063 - val_accuracy: 0.6100 - val_loss: 0.9743\n",
      "Evaluating on test set...\n",
      "Saving Keras weights to: results/lstm_cells_128_weights.npz\n",
      "  Processing layer: embedding - 1 weight arrays\n",
      "  Processing layer: lstm_0 - 3 weight arrays\n",
      "  Processing layer: classification - 2 weight arrays\n",
      "  Saved 3 weight arrays successfully\n",
      "  Layers saved: ['embedding', 'classification']\n",
      "\n",
      "Results for lstm_cells_128:\n",
      "  Training time: 4.60 seconds\n",
      "  Test Accuracy: 0.6075\n",
      "  Test F1-Score (macro): 0.4730\n",
      "  Valid F1-Score (macro): 0.4710\n",
      "  Weights saved to: results/lstm_cells_128_weights.npz\n",
      " lstm_cells_128 completed successfully!\n",
      "\n",
      "============================================================\n",
      "Training: lstm_cells_256\n",
      "Config: {'vocab_size': 2836, 'embedding_dim': 64, 'lstm_units': 256, 'num_classes': 3, 'max_length': 50, 'activation': 'tanh', 'dropout_rate': 0.2, 'learning_rate': 0.001, 'batch_size': 32, 'epochs': 15, 'num_lstm_layers': 1, 'bidirectional': False}\n",
      "============================================================\n",
      "Model created with 510,979 parameters\n",
      "Starting training for 15 epochs...\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ganadipa/code/kuliah/sem6/Machine-learning/Tugas-Besar-II-ML/.venv/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - accuracy: 0.3993 - loss: 1.0945 - val_accuracy: 0.3800 - val_loss: 1.0871\n",
      "Epoch 2/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.3679 - loss: 1.0785 - val_accuracy: 0.4700 - val_loss: 1.0233\n",
      "Epoch 3/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.4989 - loss: 0.9523 - val_accuracy: 0.6000 - val_loss: 0.9648\n",
      "Epoch 4/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.5972 - loss: 0.8302 - val_accuracy: 0.5200 - val_loss: 0.9636\n",
      "Epoch 5/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.6541 - loss: 0.7490 - val_accuracy: 0.6100 - val_loss: 0.9201\n",
      "Epoch 6/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.6913 - loss: 0.6600 - val_accuracy: 0.5500 - val_loss: 1.0314\n",
      "Epoch 7/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.5356 - loss: 0.9193 - val_accuracy: 0.3800 - val_loss: 1.0850\n",
      "Epoch 8/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.3932 - loss: 1.0748 - val_accuracy: 0.3800 - val_loss: 1.0831\n",
      "Epoch 9/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.3523 - loss: 1.1282 - val_accuracy: 0.3800 - val_loss: 1.0857\n",
      "Epoch 10/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.3465 - loss: 1.1102 - val_accuracy: 0.3800 - val_loss: 1.0833\n",
      "Evaluating on test set...\n",
      "Saving Keras weights to: results/lstm_cells_256_weights.npz\n",
      "  Processing layer: embedding - 1 weight arrays\n",
      "  Processing layer: lstm_0 - 3 weight arrays\n",
      "  Processing layer: classification - 2 weight arrays\n",
      "  Saved 3 weight arrays successfully\n",
      "  Layers saved: ['embedding', 'classification']\n",
      "\n",
      "Results for lstm_cells_256:\n",
      "  Training time: 8.04 seconds\n",
      "  Test Accuracy: 0.6050\n",
      "  Test F1-Score (macro): 0.4664\n",
      "  Valid F1-Score (macro): 0.4679\n",
      "  Weights saved to: results/lstm_cells_256_weights.npz\n",
      " lstm_cells_256 completed successfully!\n",
      "Cells    Accuracy   F1-Score   Time (s)  \n",
      "----------------------------------------\n",
      "16       0.6525    0.5013    3.4\n",
      "32       0.6200    0.4734    3.4\n",
      "64       0.5950    0.4574    3.8\n",
      "128      0.6075    0.4730    4.6\n",
      "256      0.6050    0.4664    8.0\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "cell_counts = [16, 32, 64, 128, 256]\n",
    "cell_results = {}\n",
    "\n",
    "for num_cells in cell_counts:\n",
    "    config = keras_experiment.base_config.copy()\n",
    "    config.update({\n",
    "        'num_lstm_layers': 1,\n",
    "        'bidirectional': False, \n",
    "        'lstm_units': num_cells\n",
    "    })\n",
    "    experiment_name = f\"lstm_cells_{num_cells}\"\n",
    "    \n",
    "    try:\n",
    "        result = keras_experiment.train_and_evaluate(config, experiment_name)\n",
    "        cell_results[num_cells] = result\n",
    "        print(f\" {experiment_name} completed successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\" Error in {experiment_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"{'Cells':<8} {'Accuracy':<10} {'F1-Score':<10} {'Time (s)':<10}\")\n",
    "print(\"-\"*40)\n",
    "for cells in sorted(cell_results.keys()):\n",
    "    result = cell_results[cells]\n",
    "    print(f\"{cells:<8} {result['test_accuracy']:.4f}    {result['test_f1_score']:.4f}    {result['training_time']:.1f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f08277c",
   "metadata": {},
   "source": [
    "## 4.3. Comparing Bidirectional vs. Unidirectional Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5763f008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training: lstm_ltypes_not unidirectional\n",
      "Config: {'vocab_size': 2836, 'embedding_dim': 64, 'lstm_units': 64, 'num_classes': 3, 'max_length': 50, 'activation': 'tanh', 'dropout_rate': 0.2, 'learning_rate': 0.001, 'batch_size': 32, 'epochs': 15, 'num_lstm_layers': 1, 'bidirectional': False}\n",
      "============================================================\n",
      "Model created with 214,723 parameters\n",
      "Starting training for 15 epochs...\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ganadipa/code/kuliah/sem6/Machine-learning/Tugas-Besar-II-ML/.venv/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.3686 - loss: 1.0971 - val_accuracy: 0.3700 - val_loss: 1.0841\n",
      "Epoch 2/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.3950 - loss: 1.0878 - val_accuracy: 0.3800 - val_loss: 1.0792\n",
      "Epoch 3/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4254 - loss: 1.0737 - val_accuracy: 0.5100 - val_loss: 1.0066\n",
      "Epoch 4/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4726 - loss: 0.9811 - val_accuracy: 0.5200 - val_loss: 0.9588\n",
      "Epoch 5/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5746 - loss: 0.8601 - val_accuracy: 0.6000 - val_loss: 0.8726\n",
      "Epoch 6/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6684 - loss: 0.6973 - val_accuracy: 0.6000 - val_loss: 0.9774\n",
      "Epoch 7/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7064 - loss: 0.5931 - val_accuracy: 0.6000 - val_loss: 0.8925\n",
      "Epoch 8/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7325 - loss: 0.5390 - val_accuracy: 0.5300 - val_loss: 1.2004\n",
      "Epoch 9/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6833 - loss: 0.6902 - val_accuracy: 0.5600 - val_loss: 1.4582\n",
      "Epoch 10/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7455 - loss: 0.5740 - val_accuracy: 0.5600 - val_loss: 1.1353\n",
      "Evaluating on test set...\n",
      "Saving Keras weights to: results/lstm_ltypes_not unidirectional_weights.npz\n",
      "  Processing layer: embedding - 1 weight arrays\n",
      "  Processing layer: lstm_0 - 3 weight arrays\n",
      "  Processing layer: classification - 2 weight arrays\n",
      "  Saved 3 weight arrays successfully\n",
      "  Layers saved: ['embedding', 'classification']\n",
      "\n",
      "Results for lstm_ltypes_not unidirectional:\n",
      "  Training time: 3.67 seconds\n",
      "  Test Accuracy: 0.5950\n",
      "  Test F1-Score (macro): 0.4574\n",
      "  Valid F1-Score (macro): 0.4630\n",
      "  Weights saved to: results/lstm_ltypes_not unidirectional_weights.npz\n",
      " lstm_ltypes_not unidirectional completed successfully!\n",
      "\n",
      "============================================================\n",
      "Training: lstm_ltypes_bidirectional\n",
      "Config: {'vocab_size': 2836, 'embedding_dim': 64, 'lstm_units': 64, 'num_classes': 3, 'max_length': 50, 'activation': 'tanh', 'dropout_rate': 0.2, 'learning_rate': 0.001, 'batch_size': 32, 'epochs': 15, 'num_lstm_layers': 1, 'bidirectional': True}\n",
      "============================================================\n",
      "Model created with 247,939 parameters\n",
      "Starting training for 15 epochs...\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ganadipa/code/kuliah/sem6/Machine-learning/Tugas-Besar-II-ML/.venv/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.3829 - loss: 1.0917 - val_accuracy: 0.4500 - val_loss: 1.0573\n",
      "Epoch 2/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.4845 - loss: 1.0033 - val_accuracy: 0.5600 - val_loss: 0.9533\n",
      "Epoch 3/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6284 - loss: 0.8287 - val_accuracy: 0.5700 - val_loss: 0.9066\n",
      "Epoch 4/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7455 - loss: 0.6815 - val_accuracy: 0.6400 - val_loss: 0.8058\n",
      "Epoch 5/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8597 - loss: 0.4432 - val_accuracy: 0.7300 - val_loss: 0.7314\n",
      "Epoch 6/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9538 - loss: 0.2178 - val_accuracy: 0.7700 - val_loss: 0.7170\n",
      "Epoch 7/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9786 - loss: 0.1163 - val_accuracy: 0.7800 - val_loss: 0.7230\n",
      "Epoch 8/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9959 - loss: 0.0626 - val_accuracy: 0.7300 - val_loss: 0.8189\n",
      "Epoch 9/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9937 - loss: 0.0441 - val_accuracy: 0.7800 - val_loss: 0.7256\n",
      "Epoch 10/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9994 - loss: 0.0243 - val_accuracy: 0.7700 - val_loss: 0.8310\n",
      "Epoch 11/15\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9975 - loss: 0.0176 - val_accuracy: 0.7900 - val_loss: 0.8564\n",
      "Evaluating on test set...\n",
      "Saving Keras weights to: results/lstm_ltypes_bidirectional_weights.npz\n",
      "  Processing layer: embedding - 1 weight arrays\n",
      "  Processing layer: bidirectional_lstm_0 - 6 weight arrays\n",
      "  Processing layer: classification - 2 weight arrays\n",
      "  Saved 9 weight arrays successfully\n",
      "  Layers saved: ['embedding', 'bidirectional_lstm_0', 'classification']\n",
      "\n",
      "Results for lstm_ltypes_bidirectional:\n",
      "  Training time: 5.14 seconds\n",
      "  Test Accuracy: 0.7675\n",
      "  Test F1-Score (macro): 0.7593\n",
      "  Valid F1-Score (macro): 0.7662\n",
      "  Weights saved to: results/lstm_ltypes_bidirectional_weights.npz\n",
      " lstm_ltypes_bidirectional completed successfully!\n",
      "ltypes   Accuracy   F1-Score   Time (s)  \n",
      "----------------------------------------\n",
      "0        0.5950    0.4574    3.7\n",
      "1        0.7675    0.7593    5.1\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "ltype_results = {}\n",
    "ltype_vars = [False, True]\n",
    "\n",
    "for ltype in ltype_vars:\n",
    "    config = keras_experiment.base_config.copy()\n",
    "    config.update({\n",
    "        'num_lstm_layers': 1,\n",
    "        'bidirectional': ltype, \n",
    "        'lstm_units': 64\n",
    "    })\n",
    "    experiment_name = f\"lstm_ltypes_{'bidirectional' if ltype else 'not unidirectional'}\"\n",
    "    \n",
    "    try:\n",
    "        result = keras_experiment.train_and_evaluate(config, experiment_name)\n",
    "        ltype_results[ltype] = result\n",
    "        print(f\" {experiment_name} completed successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\" Error in {experiment_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"{'ltypes':<8} {'Accuracy':<10} {'F1-Score':<10} {'Time (s)':<10}\")\n",
    "print(\"-\"*40)\n",
    "for ltypes in sorted(ltype_results.keys()):\n",
    "    result = ltype_results[ltypes]\n",
    "    print(f\"{ltypes:<8} {result['test_accuracy']:.4f}    {result['test_f1_score']:.4f}    {result['training_time']:.1f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
